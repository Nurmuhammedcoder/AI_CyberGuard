import os
import sys
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns
import time
import joblib
import json
from sklearn.datasets import make_classification
import sys
import locale
locale.getpreferredencoding = lambda: "UTF-8"
if sys.stdout.encoding != 'UTF-8':
    sys.stdout.reconfigure(encoding='utf-8')
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ TensorFlow
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.metrics import Precision, Recall, AUC
    TENSORFLOW_AVAILABLE = True
    print(f"TensorFlow —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –í–µ—Ä—Å–∏—è: {tf.__version__}")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"TensorFlow –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    print("–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ Random Forest")

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs('../models', exist_ok=True)
os.makedirs('../images', exist_ok=True)
os.makedirs('../reports', exist_ok=True)
os.makedirs('../data', exist_ok=True)

def create_sample_data():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")
    X, y = make_classification(
        n_samples=10000,
        n_features=30,
        n_informative=20,
        n_redundant=10,
        n_clusters_per_class=1,
        weights=[0.99, 0.01],
        flip_y=0.01,
        random_state=42
    )
    
    feature_names = [f'V{i}' for i in range(1, 31)]
    df = pd.DataFrame(X, columns=feature_names)
    df['Class'] = y
    
    return df

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        try:
            data = pd.read_csv('../data/creditcard.csv')
            print("–ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ creditcard.csv")
        except FileNotFoundError:
            print("–§–∞–π–ª creditcard.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ...")
            data = create_sample_data()
            data.to_csv('../data/creditcard.csv', index=False)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        X = data.drop('Class', axis=1).values
        y = data['Class'].values
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler
        joblib.dump(scaler, '../models/scaler.pkl')
        
        print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
        print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {np.bincount(y_train)}")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {np.bincount(y_test)}")
        
        return X_train, X_test, y_train, y_test
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None, None, None, None

def balance_data(X, y):
    """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è"""
    try:
        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
        weight_dict = {i: class_weights[i] for i in range(len(class_weights))}
        
        print(f"–ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y)}")
        print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {weight_dict}")
        
        return X, y, weight_dict
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return X, y, {0: 1, 1: 1}

def cross_validate_model(model, X, y, cv=5, model_type='rf'):
    """–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏"""
    try:
        if model_type == 'nn' and TENSORFLOW_AVAILABLE:
            # –î–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
            accuracies = []
            precisions = []
            recalls = []
            f1_scores = []
            
            for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):
                print(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: –§–æ–ª–¥ {fold+1}/{cv}")
                X_train, X_test = X[train_idx], X[test_idx]
                y_train, y_test = y[train_idx], y[test_idx]
                
                # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                X_train, X_val, y_train, y_val = train_test_split(
                    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42
                )
                
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
                _, _, class_weights = balance_data(X_train, y_train)
                
                # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
                nn_model = ImprovedNeuralNetwork(X_train.shape[1])
                nn_model.train(X_train, y_train, X_val, y_val, epochs=30, batch_size=256, class_weight=class_weights)
                
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
                metrics, _ = nn_model.evaluate(X_test, y_test)
                
                accuracies.append(metrics['accuracy'])
                precisions.append(metrics['precision'])
                recalls.append(metrics['recall'])
                f1_scores.append(metrics['f1'])
            
            return {
                'accuracy': (np.mean(accuracies), np.std(accuracies)),
                'precision': (np.mean(precisions), np.std(precisions)),
                'recall': (np.mean(recalls), np.std(recalls)),
                'f1': (np.mean(f1_scores), np.std(f1_scores))
            }
        else:
            # –î–ª—è Random Forest –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é
            scoring = ['accuracy', 'precision', 'recall', 'f1']
            results = {}
            
            for score in scoring:
                try:
                    cv_scores = cross_val_score(model, X, y, scoring=score, cv=cv, n_jobs=-1)
                    results[score] = (np.mean(cv_scores), np.std(cv_scores))
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ {score}: {e}")
                    results[score] = (0.0, 0.0)
            
            return results
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        return {
            'accuracy': (0.0, 0.0),
            'precision': (0.0, 0.0),
            'recall': (0.0, 0.0),
            'f1': (0.0, 0.0)
        }

def create_comprehensive_report(results, y_test):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    try:
        report = "–ü–û–õ–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–´–• –ú–û–î–ï–õ–ï–ô\n"
        report += "=" * 60 + "\n\n"
        
        for model_name, result in results.items():
            report += f"{model_name}:\n"
            report += f"  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result['time']:.2f} —Å–µ–∫—É–Ω–¥\n"
            report += f"  Accuracy: {result['metrics']['accuracy']:.4f}\n"
            report += f"  Precision: {result['metrics']['precision']:.4f}\n"
            report += f"  Recall: {result['metrics']['recall']:.4f}\n"
            report += f"  F1-Score: {result['metrics']['f1']:.4f}\n\n"
            
            report += "  –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (5-fold):\n"
            cv = result['cv_scores']
            report += f"    Accuracy: {cv['accuracy'][0]:.4f} (¬±{cv['accuracy'][1]:.4f})\n"
            report += f"    Precision: {cv['precision'][0]:.4f} (¬±{cv['precision'][1]:.4f})\n"
            report += f"    Recall: {cv['recall'][0]:.4f} (¬±{cv['recall'][1]:.4f})\n"
            report += f"    F1-Score: {cv['f1'][0]:.4f} (¬±{cv['f1'][1]:.4f})\n\n"
            
            report += "  –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n"
            report += result['metrics']['classification_report'] + "\n"
            
            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            report += "  –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:\n"
            report += f"  {result['metrics']['confusion_matrix']}\n\n"
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        models = list(results.keys())
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
        accuracies = [results[model_name]['metrics']['accuracy'] for model_name in models]
        cv_accuracies = [results[model_name]['cv_scores']['accuracy'][0] for model_name in models]
        cv_errors = [results[model_name]['cv_scores']['accuracy'][1] for model_name in models]
        
        x_pos = np.arange(len(models))
        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']
        
        axes[0, 0].bar(x_pos, accuracies, color=colors[:len(models)])
        axes[0, 0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_ylim(0, 1)
        axes[0, 0].set_xticks(x_pos)
        axes[0, 0].set_xticklabels(models, rotation=45, ha='right')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, v in enumerate(accuracies):
            axes[0, 0].text(i, v + 0.01, f'{v:.4f}', ha='center')
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1-Score
        f1_scores = [results[model_name]['metrics']['f1'] for model_name in models]
        
        axes[0, 1].bar(x_pos, f1_scores, color=colors[:len(models)])
        axes[0, 1].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1-Score –º–æ–¥–µ–ª–µ–π')
        axes[0, 1].set_ylabel('F1-Score')
        axes[0, 1].set_ylim(0, 1)
        axes[0, 1].set_xticks(x_pos)
        axes[0, 1].set_xticklabels(models, rotation=45, ha='right')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, v in enumerate(f1_scores):
            axes[0, 1].text(i, v + 0.01, f'{v:.4f}', ha='center')
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
        times = [results[model_name]['time'] for model_name in models]
        axes[1, 0].bar(x_pos, times, color=colors[:len(models)])
        axes[1, 0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)')
        axes[1, 0].set_ylabel('–í—Ä–µ–º—è (—Å–µ–∫)')
        axes[1, 0].set_xticks(x_pos)
        axes[1, 0].set_xticklabels(models, rotation=45, ha='right')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, v in enumerate(times):
            axes[1, 0].text(i, v + 0.1, f'{v:.2f}', ha='center')
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        axes[1, 1].bar(x_pos, cv_accuracies, yerr=cv_errors, 
                      color=colors[:len(models)], capsize=10)
        axes[1, 1].set_title('–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (5-fold)')
        axes[1, 1].set_ylabel('Accuracy')
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].set_xticks(x_pos)
        axes[1, 1].set_xticklabels(models, rotation=45, ha='right')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, v in enumerate(cv_accuracies):
            axes[1, 1].text(i, v + 0.01, f'{v:.4f}', ha='center')
        
        plt.tight_layout()
        plt.savefig('../images/improved_model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        with open('../reports/improved_model_comparison_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON
        json_results = {}
        for model_name, result in results.items():
            json_results[model_name] = {
                'metrics': {k: float(v) if isinstance(v, (np.integer, np.floating)) else str(v) 
                           for k, v in result['metrics'].items() if k != 'confusion_matrix'},
                'time': float(result['time']),
                'cv_scores': {k: [float(v[0]), float(v[1])] for k, v in result['cv_scores'].items()}
            }
        
        with open('../reports/improved_model_comparison.json', 'w') as f:
            json.dump(json_results, f, indent=4)
        
        return report, fig
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        return f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}", None

# –ö–ª–∞—Å—Å ImprovedNeuralNetwork (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ TensorFlow –¥–æ—Å—Ç—É–ø–µ–Ω)
if TENSORFLOW_AVAILABLE:
    class ImprovedNeuralNetwork:
        def __init__(self, input_shape):
            self.input_shape = input_shape
            self.model = self._build_model()
            self.history = None
            
        def _build_model(self):
            """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
            try:
                model = Sequential([
                    Dense(256, activation='relu', input_shape=(self.input_shape,)),
                    BatchNormalization(),
                    Dropout(0.3),
                    Dense(128, activation='relu'),
                    BatchNormalization(),
                    Dropout(0.3),
                    Dense(64, activation='relu'),
                    BatchNormalization(),
                    Dropout(0.3),
                    Dense(32, activation='relu'),
                    Dropout(0.2),
                    Dense(1, activation='sigmoid')
                ])
                
                model.compile(
                    optimizer=Adam(learning_rate=0.001),
                    loss='binary_crossentropy',
                    metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]
                )
                
                return model
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
                import traceback
                traceback.print_exc()
                return None
        
        def train(self, X_train, y_train, X_val=None, y_val=None, epochs=50, batch_size=256, class_weight=None):
            """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å callback'–∞–º–∏"""
            try:
                callbacks = []
                callbacks.append(EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1))
                callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1))
                
                validation_data = (X_val, y_val) if X_val is not None and y_val is not None else None
                
                self.history = self.model.fit(
                    X_train, y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=validation_data,
                    callbacks=callbacks,
                    verbose=1,
                    class_weight=class_weight
                )
                
                return self.history
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {e}")
                import traceback
                traceback.print_exc()
                return None
        
        def evaluate(self, X_test, y_test):
            """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
            try:
                y_pred_proba = self.model.predict(X_test, verbose=0)
                y_pred = (y_pred_proba > 0.5).astype(int).flatten()
                
                metrics = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred, zero_division=0),
                    'recall': recall_score(y_test, y_pred, zero_division=0),
                    'f1': f1_score(y_test, y_pred, zero_division=0),
                    'confusion_matrix': str(confusion_matrix(y_test, y_pred)),
                    'classification_report': classification_report(y_test, y_pred, zero_division=0)
                }
                
                return metrics, y_pred
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                return {}, []
        
        def plot_training_history(self):
            """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
            if self.history is None:
                print("–ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")
                return None
            
            try:
                fig, axes = plt.subplots(2, 2, figsize=(15, 10))
                
                # Loss
                axes[0, 0].plot(self.history.history['loss'], label='Training Loss')
                if 'val_loss' in self.history.history:
                    axes[0, 0].plot(self.history.history['val_loss'], label='Validation Loss')
                axes[0, 0].set_title('Model Loss')
                axes[0, 0].set_ylabel('Loss')
                axes[0, 0].set_xlabel('Epoch')
                axes[0, 0].legend()
                
                # Accuracy
                axes[0, 1].plot(self.history.history['accuracy'], label='Training Accuracy')
                if 'val_accuracy' in self.history.history:
                    axes[0, 1].plot(self.history.history['val_accuracy'], label='Validation Accuracy')
                axes[0, 1].set_title('Model Accuracy')
                axes[0, 1].set_ylabel('Accuracy')
                axes[0, 1].set_xlabel('Epoch')
                axes[0, 1].legend()
                
                # Precision
                if 'precision' in self.history.history:
                    axes[1, 0].plot(self.history.history['precision'], label='Training Precision')
                if 'val_precision' in self.history.history:
                    axes[1, 0].plot(self.history.history['val_precision'], label='Validation Precision')
                axes[1, 0].set_title('Model Precision')
                axes[1, 0].set_ylabel('Precision')
                axes[1, 0].set_xlabel('Epoch')
                axes[1, 0].legend()
                
                # Recall
                if 'recall' in self.history.history:
                    axes[1, 1].plot(self.history.history['recall'], label='Training Recall')
                if 'val_recall' in self.history.history:
                    axes[1, 1].plot(self.history.history['val_recall'], label='Validation Recall')
                axes[1, 1].set_title('Model Recall')
                axes[1, 1].set_ylabel('Recall')
                axes[1, 1].set_xlabel('Epoch')
                axes[1, 1].legend()
                
                plt.tight_layout()
                plt.savefig('../images/improved_nn_training.png', dpi=300, bbox_inches='tight')
                plt.close()
                
                return fig
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è: {e}")
                return None

def compare_models_improved(X_train, y_train, X_test, y_test):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    results = {}
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        _, _, class_weights = balance_data(X_train, y_train)
        
        # 1. –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ TensorFlow –¥–æ—Å—Ç—É–ø–µ–Ω)
        if TENSORFLOW_AVAILABLE:
            try:
                print("–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏...")
                nn_model = ImprovedNeuralNetwork(X_train.shape[1])
                
                if nn_model.model is None:
                    print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –º–æ–¥–µ–ª—å")
                else:
                    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(
                        X_train, y_train, test_size=0.2, stratify=y_train, random_state=42
                    )
                    
                    start_time = time.time()
                    nn_model.train(X_train_nn, y_train_nn, X_val_nn, y_val_nn, epochs=50, batch_size=256, class_weight=class_weights)
                    nn_time = time.time() - start_time
                    
                    nn_metrics, y_pred_nn = nn_model.evaluate(X_test, y_test)
                    results['Improved Neural Network'] = {
                        'metrics': nn_metrics,
                        'time': nn_time,
                        'predictions': y_pred_nn,
                        'cv_scores': cross_validate_model(None, X_train, y_train, model_type='nn')
                    }
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
                    nn_model.plot_training_history()
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏
                    try:
                        nn_model.model.save('../models/improved_neural_network.h5')
                        print("–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {e}")
        
        # 2. Random Forest —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        try:
            print("–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ Random Forest...")
            rf_model = RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                bootstrap=True,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
            )
            
            start_time = time.time()
            rf_model.fit(X_train, y_train)
            rf_time = time.time() - start_time
            
            y_pred_rf = rf_model.predict(X_test)
            rf_metrics = {
                'accuracy': accuracy_score(y_test, y_pred_rf),
                'precision': precision_score(y_test, y_pred_rf, zero_division=0),
                'recall': recall_score(y_test, y_pred_rf, zero_division=0),
                'f1': f1_score(y_test, y_pred_rf, zero_division=0),
                'confusion_matrix': str(confusion_matrix(y_test, y_pred_rf)),
                'classification_report': classification_report(y_test, y_pred_rf, zero_division=0)
            }
            
            results['Improved Random Forest'] = {
                'metrics': rf_metrics,
                'time': rf_time,
                'predictions': y_pred_rf,
                'cv_scores': cross_validate_model(rf_model, X_train, y_train, model_type='rf')
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Random Forest –º–æ–¥–µ–ª–∏
            joblib.dump(rf_model, '../models/improved_random_forest.pkl')
            print("–£–ª—É—á—à–µ–Ω–Ω–∞—è Random Forest –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è Random Forest: {e}")
        
        return results
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return {}

def print_system_info():
    """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–∞—Ö"""
    print("–°–ò–°–¢–ï–ú–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
    print("=" * 50)
    print(f"Python –≤–µ—Ä—Å–∏—è: {sys.version}")
    print(f"–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º—ã: {os.name}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
    libraries_to_check = [
        'tensorflow', 'numpy', 'pandas', 'sklearn', 
        'matplotlib', 'seaborn', 'joblib'
    ]
    
    print("\n–í–µ—Ä—Å–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫:")
    for lib in libraries_to_check:
        try:
            module = __import__(lib)
            version = getattr(module, '__version__', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            print(f"  {lib}: {version}")
        except ImportError:
            print(f"  {lib}: –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    
    print()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        print("–ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´ –°–†–ê–í–ù–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô")
        print("=" * 60)
        
        # –í—ã–≤–æ–¥–∏–º —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        print_system_info()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\n–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
        print("-" * 30)
        X_train, X_test, y_train, y_test = load_data()
        
        if X_train is None:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            return
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        print("\n–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("-" * 30)
        results = compare_models_improved(X_train, y_train, X_test, y_test)
        
        if results:
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
            print("\n–°–û–ó–î–ê–ù–ò–ï –û–¢–ß–ï–¢–ê")
            print("-" * 30)
            report, fig = create_comprehensive_report(results, y_test)
            print(report)
            
            print("\n" + "=" * 60)
            print("–°–†–ê–í–ù–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–´—Ö –ú–û–î–ï–õ–ï–ô –ó–ê–í–ï–†–®–ï–ù–û!")
            print("=" * 60)
            print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–∞–ø–∫–∞—Ö:")
            print("  üìÅ models/ - –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏")
            print("  üìÅ images/ - –ì—Ä–∞—Ñ–∏–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            print("  üìÅ reports/ - –¢–µ–∫—Å—Ç–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã –∏ JSON")
            
            if TENSORFLOW_AVAILABLE:
                print("\n‚úì TensorFlow/Keras: –ü–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã")
            else:
                print("\n‚ö†Ô∏è  TensorFlow/Keras: –ù–µ–¥–æ—Å—Ç—É–ø–Ω—ã (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ Random Forest)")
                
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.")
            print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º.")
            
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()