#!/usr/bin/env python3
"""
AI CyberGuard - –ú–æ–¥—É–ª—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –æ—à–∏–±–æ–∫
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
import joblib
import json
from datetime import datetime

# –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report)
from sklearn.datasets import make_classification
from sklearn.utils.class_weight import compute_class_weight

# TensorFlow –∏–º–ø–æ—Ä—Ç
TENSORFLOW_AVAILABLE = False
try:
    import tensorflow as tf
    tf.get_logger().setLevel('ERROR')
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.metrics import Precision, Recall, AUC
    TENSORFLOW_AVAILABLE = True
    print(f"‚úì TensorFlow {tf.__version__} –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
except Exception as e:
    print(f"‚ö†Ô∏è TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    TENSORFLOW_AVAILABLE = False

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
def create_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    directories = ['models', 'images', 'reports', 'data']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"‚úì –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory} —Å–æ–∑–¥–∞–Ω–∞")

def create_sample_data(n_samples=10000):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞
    np.random.seed(42)
    
    # –ë–∞–∑–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ vs –∞—Ç–∞–∫–∏
    X, y = make_classification(
        n_samples=n_samples,
        n_features=30,
        n_informative=20,
        n_redundant=10,
        n_classes=2,
        weights=[0.85, 0.15],  # 85% –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π, 15% –∞—Ç–∞–∫–∏
        random_state=42
    )
    
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_names = [
        'duration', 'protocol_type', 'service', 'flag', 'src_bytes',
        'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',
        'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',
        'su_attempted', 'num_root', 'num_file_creations', 'num_shells',
        'num_access_files', 'num_outbound_cmds', 'is_host_login',
        'is_guest_login', 'count', 'srv_count', 'serror_rate',
        'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',
        'diff_srv_rate'
    ]
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(X, columns=feature_names)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    attack_types = ['normal', 'dos', 'probe', 'r2l', 'u2r']
    # –°–æ–∑–¥–∞–µ–º —Ç–∏–ø—ã –∞—Ç–∞–∫
    attack_types = []
    for label in y:
        if label == 0:
            attack_types.append('normal')
        else:
            attack_types.append(np.random.choice(['dos', 'probe', 'r2l', 'u2r']))
    df['attack_type'] = attack_types
    df['is_attack'] = y
    
    print(f"–°–æ–∑–¥–∞–Ω–æ {n_samples} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
    print(f"–ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫: {sum(y == 0)} ({sum(y == 0)/n_samples*100:.1f}%)")
    print(f"–ê—Ç–∞–∫–∏: {sum(y == 1)} ({sum(y == 1)/n_samples*100:.1f}%)")
    
    return df

def preprocess_data(data):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
    
    # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    if 'is_attack' in numeric_features:
        numeric_features.remove('is_attack')
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    X = data[numeric_features].fillna(0)
    y = data['is_attack'].values
    
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numeric_features)}")
    
    return X, y, numeric_features

class ImprovedRandomForest:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Random Forest"""
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.feature_names = None
        
    def train(self, X_train, y_train, X_val=None, y_val=None):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print("–û–±—É—á–µ–Ω–∏–µ Random Forest...")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            bootstrap=True,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        start_time = time.time()
        self.model.fit(X_train_scaled, y_train)
        training_time = time.time() - start_time
        
        print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        return training_time
    
    def evaluate(self, X_test, y_test):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        if self.model is None or self.scaler is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        X_test_scaled = self.scaler.transform(X_test)
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'classification_report': classification_report(y_test, y_pred, zero_division=0)
        }
        
        return metrics, y_pred, y_pred_proba

class ImprovedNeuralNetwork:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å"""
    
    def __init__(self, input_shape):
        self.input_shape = input_shape
        self.model = None
        self.scaler = None
        self.history = None
        
        if TENSORFLOW_AVAILABLE:
            self._build_model()
        else:
            raise ImportError("TensorFlow –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
    
    def _build_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        self.model = Sequential([
            Dense(128, activation='relu', input_shape=(self.input_shape,)),
            BatchNormalization(),
            Dropout(0.3),
            
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            
            Dense(32, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),
            
            Dense(16, activation='relu'),
            Dropout(0.1),
            
            Dense(1, activation='sigmoid')
        ])
        
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]
        )
        
        print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å–æ–∑–¥–∞–Ω–∞")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {self.model.count_params()}")
    
    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=128):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        print("–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        # Callback'–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        start_time = time.time()
        self.history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=(X_val_scaled, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        training_time = time.time() - start_time
        
        print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        return training_time
    
    def evaluate(self, X_test, y_test):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        if self.model is None or self.scaler is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        X_test_scaled = self.scaler.transform(X_test)
        y_pred_proba = self.model.predict(X_test_scaled, verbose=0).flatten()
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'classification_report': classification_report(y_test, y_pred, zero_division=0)
        }
        
        return metrics, y_pred, y_pred_proba
    
    def plot_training_history(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if self.history is None:
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(self.history.history['loss'], label='Training Loss')
        axes[0, 0].plot(self.history.history['val_loss'], label='Validation Loss')
        axes[0, 0].set_title('Model Loss')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Accuracy
        axes[0, 1].plot(self.history.history['accuracy'], label='Training Accuracy')
        axes[0, 1].plot(self.history.history['val_accuracy'], label='Validation Accuracy')
        axes[0, 1].set_title('Model Accuracy')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # Precision
        if 'precision' in self.history.history:
            axes[1, 0].plot(self.history.history['precision'], label='Training Precision')
            axes[1, 0].plot(self.history.history['val_precision'], label='Validation Precision')
            axes[1, 0].set_title('Model Precision')
            axes[1, 0].set_ylabel('Precision')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
        
        # Recall
        if 'recall' in self.history.history:
            axes[1, 1].plot(self.history.history['recall'], label='Training Recall')
            axes[1, 1].plot(self.history.history['val_recall'], label='Validation Recall')
            axes[1, 1].set_title('Model Recall')
            axes[1, 1].set_ylabel('Recall')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig('images/neural_network_training.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return fig

def compare_models(X_train, X_test, y_train, y_test):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
    print("\n–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)
    
    results = {}
    
    # 1. Random Forest
    print("\n1. Random Forest")
    print("-" * 30)
    rf_model = ImprovedRandomForest()
    rf_time = rf_model.train(X_train, y_train)
    rf_metrics, rf_pred, rf_proba = rf_model.evaluate(X_test, y_test)
    
    results['Random Forest'] = {
        'model': rf_model,
        'metrics': rf_metrics,
        'training_time': rf_time,
        'predictions': rf_pred,
        'probabilities': rf_proba
    }
    
    print(f"Accuracy: {rf_metrics['accuracy']:.4f}")
    print(f"Precision: {rf_metrics['precision']:.4f}")
    print(f"Recall: {rf_metrics['recall']:.4f}")
    print(f"F1-Score: {rf_metrics['f1']:.4f}")
    
    # 2. Neural Network (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω TensorFlow)
    if TENSORFLOW_AVAILABLE:
        print("\n2. Neural Network")
        print("-" * 30)
        
        try:
            # –†–∞–∑–¥–µ–ª—è–µ–º training set –Ω–∞ train –∏ validation
            X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(
                X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
            )
            
            nn_model = ImprovedNeuralNetwork(X_train.shape[1])
            nn_time = nn_model.train(X_train_nn, y_train_nn, X_val_nn, y_val_nn, epochs=50)
            nn_metrics, nn_pred, nn_proba = nn_model.evaluate(X_test, y_test)
            
            results['Neural Network'] = {
                'model': nn_model,
                'metrics': nn_metrics,
                'training_time': nn_time,
                'predictions': nn_pred,
                'probabilities': nn_proba
            }
            
            print(f"Accuracy: {nn_metrics['accuracy']:.4f}")
            print(f"Precision: {nn_metrics['precision']:.4f}")
            print(f"Recall: {nn_metrics['recall']:.4f}")
            print(f"F1-Score: {nn_metrics['f1']:.4f}")
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            nn_model.plot_training_history()
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏: {e}")
    
    return results

def create_visualizations(results, y_test):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    models = list(results.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightyellow']
    
    for i, metric in enumerate(metrics):
        ax = axes[i//2, i%2]
        values = [results[model]['metrics'][metric] for model in models]
        
        bars = ax.bar(models, values, color=colors[:len(models)])
        ax.set_title(f'{metric.capitalize()} Comparison')
        ax.set_ylabel(metric.capitalize())
        ax.set_ylim(0, 1)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('images/model_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))
    if len(results) == 1:
        axes = [axes]
    
    for i, (model_name, result) in enumerate(results.items()):
        cm = result['metrics']['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])
        axes[i].set_title(f'{model_name}\nConfusion Matrix')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('Actual')
    
    plt.tight_layout()
    plt.savefig('images/confusion_matrices.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É 'images/'")

def save_models_and_results(results):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print("\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    for model_name, result in results.items():
        model = result['model']
        
        if model_name == 'Random Forest':
            joblib.dump(model.model, 'models/random_forest.pkl')
            joblib.dump(model.scaler, 'models/rf_scaler.pkl')
            print(f"‚úì Random Forest –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
        elif model_name == 'Neural Network' and TENSORFLOW_AVAILABLE:
            model.model.save('models/neural_network.h5')
            joblib.dump(model.scaler, 'models/nn_scaler.pkl')
            print(f"‚úì Neural Network –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON
    json_results = {}
    for model_name, result in results.items():
        json_results[model_name] = {
            'accuracy': float(result['metrics']['accuracy']),
            'precision': float(result['metrics']['precision']),
            'recall': float(result['metrics']['recall']),
            'f1': float(result['metrics']['f1']),
            'training_time': float(result['training_time'])
        }
    
    with open('reports/model_comparison.json', 'w') as f:
        json.dump(json_results, f, indent=4)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    report = "AI CYBERGUARD - –û–¢–ß–ï–¢ –û –°–†–ê–í–ù–ï–ù–ò–ò –ú–û–î–ï–õ–ï–ô\n"
    report += "=" * 50 + "\n"
    report += f"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    for model_name, result in results.items():
        report += f"{model_name}:\n"
        report += f"  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result['training_time']:.2f} —Å–µ–∫\n"
        report += f"  Accuracy: {result['metrics']['accuracy']:.4f}\n"
        report += f"  Precision: {result['metrics']['precision']:.4f}\n"
        report += f"  Recall: {result['metrics']['recall']:.4f}\n"
        report += f"  F1-Score: {result['metrics']['f1']:.4f}\n\n"
        report += "  –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n"
        report += result['metrics']['classification_report'] + "\n\n"
    
    with open('reports/detailed_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É 'reports/'")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("AI CYBERGUARD - –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    create_directories()
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        print("\n1. –°–û–ó–î–ê–ù–ò–ï –î–ê–ù–ù–´–•")
        print("-" * 30)
        data = create_sample_data(15000)
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        print("\n2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
        print("-" * 30)
        X, y, feature_names = preprocess_data(data)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
        print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("\n3. –û–ë–£–ß–ï–ù–ò–ï –ò –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("-" * 30)
        results = compare_models(X_train, X_test, y_train, y_test)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        print("\n4. –°–û–ó–î–ê–ù–ò–ï –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô")
        print("-" * 30)
        create_visualizations(results, y_test)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print("\n5. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("-" * 30)
        save_models_and_results(results)
        
        print("\n" + "=" * 60)
        print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("=" * 60)
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print("üìÅ models/ - –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        print("üìÅ images/ - –ì—Ä–∞—Ñ–∏–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏") 
        print("üìÅ reports/ - –û—Ç—á–µ—Ç—ã –∏ –º–µ—Ç—Ä–∏–∫–∏")
        
        if TENSORFLOW_AVAILABLE:
            print("\n‚úì TensorFlow: –ü–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω")
        else:
            print("\n‚ö†Ô∏è TensorFlow: –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ Random Forest)")
            
    except Exception as e:
        print(f"\n–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()